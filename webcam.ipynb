{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03b131b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background captured\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "nine\n",
      "nine\n",
      "three\n",
      "three\n",
      "three\n",
      "three\n",
      "three\n",
      "three\n",
      "three\n",
      "three\n",
      "five\n",
      "five\n",
      "nine\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "nine\n",
      "nine\n",
      "zero\n",
      "nine\n",
      "six\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "zero\n",
      "eight\n",
      "five\n",
      "five\n",
      "eight\n",
      "zero\n",
      "zero\n",
      "five\n",
      "five\n",
      "five\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "nine\n",
      "five\n",
      "five\n",
      "five\n",
      "five\n",
      "zero\n",
      "zero\n",
      "left\n",
      "left\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "down\n",
      "down\n",
      "nine\n",
      "nine\n",
      "three\n",
      "three\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "eight\n",
      "nine\n",
      "nine\n",
      "six\n",
      "zero\n",
      "zero\n",
      "nine\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "nine\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "five\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "zero\n",
      "eight\n",
      "nine\n",
      "nine\n",
      "down\n",
      "down\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "left\n",
      "nine\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "left\n",
      "nine\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "stop\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "zero\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "zero\n",
      "nine\n",
      "zero\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "zero\n",
      "nine\n",
      "left\n",
      "zero\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "left\n",
      "left\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "zero\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "left\n",
      "left\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "left\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "nine\n",
      "stop\n",
      "stop\n",
      "down\n",
      "down\n",
      "down\n",
      "down\n",
      "down\n",
      "down\n",
      "down\n",
      "down\n",
      "down\n",
      "nine\n",
      "down\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n",
      "stop\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# the dictionary that save the information of the gestures \n",
    "gesture_names = {0:'down',\n",
    "                 1:'eight',\n",
    "                 2:'five',\n",
    "                 3:'four',\n",
    "                 4:'left',\n",
    "                 5:'nine',\n",
    "                 6:'one',\n",
    "                 7:'right',\n",
    "                 8:'seven',\n",
    "                 9:'six',\n",
    "                 10:'stop',\n",
    "                 11:'three',\n",
    "                 12:'two',\n",
    "                 13:'up',\n",
    "                 14:'zero' }\n",
    "\n",
    "\n",
    "#normal NN(!!!change if use this model!!!)\n",
    "#model = load_model('C:/Users/dyson/model.h5')\n",
    "#VGG\n",
    "model = load_model('C:/Users/dyson/Model_VGG.h5')\n",
    "\n",
    "#function to predict the image with normal neural network\n",
    "def predict_rgb_image(img):\n",
    "    pred_array = model.predict(img)\n",
    "    result = gesture_names[np.argmax(pred_array[0], axis=-1)]\n",
    "    #result = gesture_names[model.predict_classes(img)[0]]\n",
    "    score = float(\"%0.2f\" % (max(pred_array[0]) * 100))\n",
    "    print(result)\n",
    "    return (result, score)\n",
    "\n",
    "#function to predict the image with VGG network\n",
    "def predict_rgb_image_vgg(image):\n",
    "    image = np.array(image, dtype='float32')\n",
    "    image /= 255\n",
    "    pred_array = model.predict(image)\n",
    "    result = gesture_names[np.argmax(pred_array)]\n",
    "    score = float(\"%0.2f\" % (max(pred_array[0]) * 100))\n",
    "    print(result)\n",
    "    return result, score\n",
    "\n",
    "\n",
    "# parameters\n",
    "prediction = ''\n",
    "action = ''\n",
    "score = 0\n",
    "img_counter = 500\n",
    "#settings\n",
    "#width of area of interest\n",
    "aoi_width = 0.5  \n",
    "#height of area of interest\n",
    "aoi_height = 0.6  \n",
    "# binary threshold\n",
    "threshold = 62 \n",
    "# GaussianBlur parameter\n",
    "blurValue = 41 \n",
    "#Background  threshold\n",
    "bgSubThreshold = 50\n",
    "learningRate = 0\n",
    "\n",
    "# variableslt\n",
    "isBgCaptured = 0  # bool, whether the background captured\n",
    "triggerSwitch = False  # if true, keyboard simulator works\n",
    "\n",
    "#function to remove the background\n",
    "def remove_background(frame):\n",
    "    fgmask = bgModel.apply(frame, learningRate=learningRate)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    fgmask = cv2.erode(fgmask, kernel, iterations=1)\n",
    "    res = cv2.bitwise_and(frame, frame, mask=fgmask)\n",
    "    return res\n",
    "\n",
    "\n",
    "#setting camera\n",
    "camera = cv2.VideoCapture(0)\n",
    "camera.set(10, 200)\n",
    "\n",
    "while camera.isOpened():\n",
    "    ret, frame = camera.read()\n",
    "    # smoothing filter\n",
    "    frame = cv2.bilateralFilter(frame, 5, 50, 100)  \n",
    "    # flip the frame horizontally to make the video have the same order with real world(like a mirror)\n",
    "    frame = cv2.flip(frame, 1) \n",
    "    #draw area of interest\n",
    "    cv2.rectangle(frame, (int(aoi_width * frame.shape[1]), 0),\n",
    "                  (frame.shape[1], int(aoi_height * frame.shape[0])), (0, 255, 0), 2)\n",
    "    #show the video with area of interest\n",
    "    cv2.imshow('WebCam', frame)\n",
    "\n",
    "    # if the background is captured,run this part\n",
    "    if isBgCaptured == 1:\n",
    "        #remove the background\n",
    "        img = remove_background(frame)\n",
    "        #get area of interest\n",
    "        img = img[0:int(aoi_width * frame.shape[0]),\n",
    "              int(aoi_height * frame.shape[1]):frame.shape[1]]\n",
    "        \n",
    "        # convert the image into greyscale image\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        #show the greyscale video\n",
    "        cv2.imshow('gray', gray)\n",
    "        #apply the Gaussianblur\n",
    "        blur = cv2.GaussianBlur(gray, (blurValue, blurValue), 0)\n",
    "        #show the blured video\n",
    "        cv2.imshow('blur', blur)\n",
    "        #apply the threshold\n",
    "        ret, thresh = cv2.threshold(blur, threshold, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        #Normal NN(!!!change if use this model!!!)\n",
    "        #target = cv2.resize(thresh, (224, 224))\n",
    "        #target = target.reshape(1, 224, 224, 1)\n",
    "        #VGG : make prediction with VGG model\n",
    "        target = np.stack((thresh,) * 3, axis=-1)\n",
    "        target = cv2.resize(target, (224, 224))\n",
    "        target = target.reshape(1, 224, 224, 3)\n",
    "        prediction,score = predict_rgb_image_vgg(target)\n",
    "        #show result with text\n",
    "        cv2.putText(thresh, f\"Prediction: {prediction} ({score}%)\", (50, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "                    (255, 255, 255))\n",
    "        #Show the result video\n",
    "        cv2.imshow('Binary', thresh)\n",
    "        \n",
    "        #copy the threshold iamges\n",
    "        thresh1 = copy.deepcopy(thresh)\n",
    "        #find contours\n",
    "        contours, hierarchy = cv2.findContours(thresh1, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        length = len(contours)\n",
    "        maxArea = -1\n",
    "        if length > 0:\n",
    "            # find the biggest contour\n",
    "            for i in range(length):  \n",
    "                temp = contours[i]\n",
    "                area = cv2.contourArea(temp)\n",
    "                if area > maxArea:\n",
    "                    maxArea = area\n",
    "                    ci = i\n",
    "\n",
    "            res = contours[ci]\n",
    "            hull = cv2.convexHull(res)\n",
    "            drawing = np.zeros(img.shape, np.uint8)\n",
    "            #draw the contours\n",
    "            cv2.drawContours(drawing, [res], 0, (0, 0, 255), 2)\n",
    "            cv2.drawContours(drawing, [hull], 0, (255, 0, 0), 3)\n",
    "        #show the video of contours\n",
    "        cv2.imshow('Contour', drawing)\n",
    "\n",
    "        \n",
    "\n",
    "    # press ESC to exit \n",
    "    k = cv2.waitKey(10)\n",
    "    if k == 27:  \n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "        \n",
    "    # press 'b' to capture the background    \n",
    "    elif k == ord('b'): \n",
    "        bgModel = cv2.createBackgroundSubtractorMOG2(0, bgSubThreshold)\n",
    "        time.sleep(2)\n",
    "        isBgCaptured = 1\n",
    "        print('Background captured')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#References:\n",
    "    #Keras Models   https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "    #Open CV2 Camera    https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html\n",
    "    #Open CV2 Image processing    https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
